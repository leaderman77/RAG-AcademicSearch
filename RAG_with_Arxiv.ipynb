{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnKe6wvtVXdUduVPkVGF5w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leaderman77/RAG-AcademicSearch/blob/main/RAG_with_Arxiv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing necessary libraries"
      ],
      "metadata": {
        "id": "CNqyFpvQ9v17"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2qHGOaAVSdlI"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q langchain openai ragas arxiv pymupdf chromadb wandb tiktoken faiss-cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Providing openai key to use openai gpt 3.5 large language model for this project"
      ],
      "metadata": {
        "id": "jcEXFAaM94vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from getpass import getpass\n",
        "\n",
        "# openai.api_key = getpass(\"Please provide your OpenAI Key: \")\n",
        "# os.environ[\"OPENAI_API_KEY\"] = openai.api_key\n",
        "os.environ[\"OPENAI_API_KEY\"] = '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'"
      ],
      "metadata": {
        "id": "QcBuLLTPSphx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting papers from Arxiv\n",
        "## Here **load_max_docs** is number of papers we want to load and this value can be changed.\n",
        "\n",
        "## User asks general questions to load list of papers relevant to his query. For example: \"k-Means cluster algorithm\""
      ],
      "metadata": {
        "id": "nWhFHmKl-Gtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import ArxivLoader\n",
        "\n",
        "base_docs = ArxivLoader(query=\"k-Means cluster algorithm\", load_max_docs=10).load()\n",
        "len(base_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OC8XH2Y-S2_w",
        "outputId": "ace4e3a2-4277-4887-d485-89bf4d7de22c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check papers metadata"
      ],
      "metadata": {
        "id": "MvlZ0pFR-LzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in base_docs:\n",
        "  print(doc.metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZPQLEVRS9HY",
        "outputId": "39fec364-7f3d-437f-daef-bff0345e9b22"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Published': '2014-10-26', 'Title': 'Notes on using Determinantal Point Processes for Clustering with Applications to Text Clustering', 'Authors': 'Apoorv Agarwal, Anna Choromanska, Krzysztof Choromanski', 'Summary': 'In this paper, we compare three initialization schemes for the KMEANS\\nclustering algorithm: 1) random initialization (KMEANSRAND), 2) KMEANS++, and\\n3) KMEANSD++. Both KMEANSRAND and KMEANS++ have a major that the value of k\\nneeds to be set by the user of the algorithms. (Kang 2013) recently proposed a\\nnovel use of determinantal point processes for sampling the initial centroids\\nfor the KMEANS algorithm (we call it KMEANSD++). They, however, do not provide\\nany evaluation establishing that KMEANSD++ is better than other algorithms. In\\nthis paper, we show that the performance of KMEANSD++ is comparable to KMEANS++\\n(both of which are better than KMEANSRAND) with KMEANSD++ having an additional\\nthat it can automatically approximate the value of k.'}\n",
            "{'Published': '2019-10-15', 'Title': 'MSD-Kmeans: A Novel Algorithm for Efficient Detection of Global and Local Outliers', 'Authors': 'Yuanyuan Wei, Julian Jang-Jaccard, Fariza Sabrina, Timothy McIntosh', 'Summary': 'Outlier detection is a technique in data mining that aims to detect unusual\\nor unexpected records in the dataset. Existing outlier detection algorithms\\nhave different pros and cons and exhibit different sensitivity to noisy data\\nsuch as extreme values. In this paper, we propose a novel cluster-based outlier\\ndetection algorithm named MSD-Kmeans that combines the statistical method of\\nMean and Standard Deviation (MSD) and the machine learning clustering algorithm\\nK-means to detect outliers more accurately with the better control of extreme\\nvalues. There are two phases in this combination method of MSD-Kmeans: (1)\\napplying MSD algorithm to eliminate as many noisy data to minimize the\\ninterference on clusters, and (2) applying K-means algorithm to obtain local\\noptimal clusters. We evaluate our algorithm and demonstrate its effectiveness\\nin the context of detecting possible overcharging of taxi fares, as greedy\\ndishonest drivers may attempt to charge high fares by detouring. We compare the\\nperformance indicators of MSD-Kmeans with those of other outlier detection\\nalgorithms, such as MSD, K-means, Z-score, MIQR and LOF, and prove that the\\nproposed MSD-Kmeans algorithm achieves the highest measure of precision,\\naccuracy, and F-measure. We conclude that MSD-Kmeans can be used for effective\\nand efficient outlier detection on data of varying quality on IoT devices.'}\n",
            "{'Published': '2023-05-01', 'Title': 'CKmeans and FCKmeans : Two deterministic initialization procedures for Kmeans algorithm using a modified crowding distance', 'Authors': 'Abdesslem Layeb', 'Summary': 'This paper presents two novel deterministic initialization procedures for\\nK-means clustering based on a modified crowding distance. The procedures, named\\nCKmeans and FCKmeans, use more crowded points as initial centroids.\\nExperimental studies on multiple datasets demonstrate that the proposed\\napproach outperforms Kmeans and Kmeans++ in terms of clustering accuracy. The\\neffectiveness of CKmeans and FCKmeans is attributed to their ability to select\\nbetter initial centroids based on the modified crowding distance. Overall, the\\nproposed approach provides a promising alternative for improving K-means\\nclustering.'}\n",
            "{'Published': '2024-02-19', 'Title': 'Kernel KMeans clustering splits for end-to-end unsupervised decision trees', 'Authors': 'Louis Ohl, Pierre-Alexandre Mattei, Mickaël Leclercq, Arnaud Droit, Frédéric Precioso', 'Summary': 'Trees are convenient models for obtaining explainable predictions on\\nrelatively small datasets. Although there are many proposals for the end-to-end\\nconstruction of such trees in supervised learning, learning a tree end-to-end\\nfor clustering without labels remains an open challenge. As most works focus on\\ninterpreting with trees the result of another clustering algorithm, we present\\nhere a novel end-to-end trained unsupervised binary tree for clustering: Kauri.\\nThis method performs a greedy maximisation of the kernel KMeans objective\\nwithout requiring the definition of centroids. We compare this model on\\nmultiple datasets with recent unsupervised trees and show that Kauri performs\\nidentically when using a linear kernel. For other kernels, Kauri often\\noutperforms the concatenation of kernel KMeans and a CART decision tree.'}\n",
            "{'Published': '2024-01-21', 'Title': 'Enabling clustering algorithms to detect clusters of varying densities through scale-invariant data preprocessing', 'Authors': 'Sunil Aryal, Jonathan R. Wells, Arbind Agrahari Baniya, KC Santosh', 'Summary': \"In this paper, we show that preprocessing data using a variant of rank\\ntransformation called 'Average Rank over an Ensemble of Sub-samples (ARES)'\\nmakes clustering algorithms robust to data representation and enable them to\\ndetect varying density clusters. Our empirical results, obtained using three\\nmost widely used clustering algorithms-namely KMeans, DBSCAN, and DP (Density\\nPeak)-across a wide range of real-world datasets, show that clustering after\\nARES transformation produces better and more consistent results.\"}\n",
            "{'Published': '2023-05-04', 'Title': 'Influence of various text embeddings on clustering performance in NLP', 'Authors': 'Rohan Saha', 'Summary': 'With the advent of e-commerce platforms, reviews are crucial for customers to\\nassess the credibility of a product. The star ratings do not always match the\\nreview text written by the customer. For example, a three star rating (out of\\nfive) may be incongruous with the review text, which may be more suitable for a\\nfive star review. A clustering approach can be used to relabel the correct star\\nratings by grouping the text reviews into individual groups. In this work, we\\nexplore the task of choosing different text embeddings to represent these\\nreviews and also explore the impact the embedding choice has on the performance\\nof various classes of clustering algorithms. We use contextual (BERT) and\\nnon-contextual (Word2Vec) text embeddings to represent the text and measure\\ntheir impact of three classes on clustering algorithms - partitioning based\\n(KMeans), single linkage agglomerative hierarchical, and density based (DBSCAN\\nand HDBSCAN), each with various experimental settings. We use the silhouette\\nscore, adjusted rand index score, and cluster purity score metrics to evaluate\\nthe performance of the algorithms and discuss the impact of different\\nembeddings on the clustering performance. Our results indicate that the type of\\nembedding chosen drastically affects the performance of the algorithm, the\\nperformance varies greatly across different types of clustering algorithms, no\\nembedding type is better than the other, and DBSCAN outperforms KMeans and\\nsingle linkage agglomerative clustering but also labels more data points as\\noutliers. We provide a thorough comparison of the performances of different\\nalgorithms and provide numerous ideas to foster further research in the domain\\nof text clustering.'}\n",
            "{'Published': '2017-10-07', 'Title': 'A New Spectral Clustering Algorithm', 'Authors': 'W. R. Casper, Balu Nadiga', 'Summary': 'We present a new clustering algorithm that is based on searching for natural\\ngaps in the components of the lowest energy eigenvectors of the Laplacian of a\\ngraph. In comparing the performance of the proposed method with a set of other\\npopular methods (KMEANS, spectral-KMEANS, and an agglomerative method) in the\\ncontext of the Lancichinetti-Fortunato-Radicchi (LFR) Benchmark for undirected\\nweighted overlapping networks, we find that the new method outperforms the\\nother spectral methods considered in certain parameter regimes. Finally, in an\\napplication to climate data involving one of the most important modes of\\ninterannual climate variability, the El Nino Southern Oscillation phenomenon,\\nwe demonstrate the ability of the new algorithm to readily identify different\\nflavors of the phenomenon.'}\n",
            "{'Published': '2017-10-09', 'Title': 'Run Time Prediction for Big Data Iterative ML Algorithms: a KMeans case study', 'Authors': 'Eduardo Rodrigues, Ricardo Morla', 'Summary': 'Data science and machine learning algorithms running on big data\\ninfrastructure are increasingly important in activities ranging from business\\nintelligence and analytics to cybersecurity, smart city management, and many\\nfields of science and engineering. As these algorithms are further integrated\\ninto daily operations, understanding how long they take to run on a big data\\ninfrastructure is paramount to controlling costs and delivery times. In this\\npaper we discuss the issues involved in understanding the run time of iterative\\nmachine learning algorithms and provide a case study of such an algorithm -\\nincluding a statistical characterization and model of the run time of an\\nimplementation of K-Means for the Spark big data engine using the Edward\\nprobabilistic programming language.'}\n",
            "{'Published': '2020-10-04', 'Title': 'Forecasting with Bayesian Grouped Random Effects in Panel Data', 'Authors': 'Boyuan Zhang', 'Summary': 'In this paper, we estimate and leverage latent constant group structure to\\ngenerate the point, set, and density forecasts for short dynamic panel data. We\\nimplement a nonparametric Bayesian approach to simultaneously identify\\ncoefficients and group membership in the random effects which are heterogeneous\\nacross groups but fixed within a group. This method allows us to flexibly\\nincorporate subjective prior knowledge on the group structure that potentially\\nimproves the predictive accuracy. In Monte Carlo experiments, we demonstrate\\nthat our Bayesian grouped random effects (BGRE) estimators produce accurate\\nestimates and score predictive gains over standard panel data estimators. With\\na data-driven group structure, the BGRE estimators exhibit comparable accuracy\\nof clustering with the Kmeans algorithm and outperform a two-step Bayesian\\ngrouped estimator whose group structure relies on Kmeans. In the empirical\\nanalysis, we apply our method to forecast the investment rate across a broad\\nrange of firms and illustrate that the estimated latent group structure\\nimproves forecasts relative to standard panel data estimators.'}\n",
            "{'Published': '2022-05-09', 'Title': 'A Hybrid Approach: Utilising Kmeans Clustering and Naive Bayes for IoT Anomaly Detection', 'Authors': 'Lincoln Best, Ernest Foo, Hui Tian', 'Summary': 'The proliferation and variety of Internet of Things devices means that they\\nhave increasingly become a viable target for malicious users. This has created\\na need for anomaly detection algorithms that can work across multiple devices.\\nThis thesis suggests a potential alternative to the current anomaly detection\\nalgorithms to be implemented within IoT systems that can be applied across\\ndifferent types of devices. This algorithm is comprised of both unsupverised\\nand supervised machine areas of machine learning combining the strongest facet\\nof each. The algorithm involves the initial k-means clustering of attacks and\\nassigns them to clusters. Next, the clusters are then used by the AdaBoosted\\nNaive Bayes supervised learning algorithm in order to teach itself which piece\\nof data should be clustered to which specific attack. This increases the\\naccuracy of the proposed algorithm by adding clustered data before the final\\nclassification step, ensuring a more accurate algorithm. The correct\\nindentification percentage scores for this proposed algorithm range anywhere\\nfrom 90% to 100%, as well as rating the proposed algorithms accuracy, precision\\nand recall. These high scores achieve an accurate, flexible, scalable,\\noptimised algorithm that could potentially be in different IoT devices,\\nensuring strong data integrity and privacy.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_docs[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "cKaQC0lAaC7X",
        "outputId": "73b0ba55-670f-4708-dc07-c8f7071ca728"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Notes on Using Determinantal Point Processes for Clustering with Applications to\\nText Clustering\\nApoorv Agarwal\\nColumbia University\\nNew York, NY, USA\\napoorv@cs.columbia.edu\\nAnna Choromanska\\nCourant Institute of Mathematical Sciences\\nNew York, NY, USA\\nachoroma@cims.nyu.edu\\nKrzysztof Choromanski\\nGoogle Research\\nNew York, NY, USA\\nkchoro@gmail.com\\nAbstract\\nIn this paper, we compare three initialization schemes\\nfor the KMEANS clustering algorithm: 1) random ini-\\ntialization (KMEANSRAND), 2) KMEANS++, and 3)\\nKMEANSD++. Both KMEANSRAND and KMEANS++\\nhave a major that the value of k needs to be set by the\\nuser of the algorithms. (Kang 2013) recently proposed a\\nnovel use of determinantal point processes for sampling\\nthe initial centroids for the KMEANS algorithm (we call\\nit KMEANSD++). They, however, do not provide any\\nevaluation establishing that KMEANSD++ is better than\\nother algorithms. In this paper, we show that the perfor-\\nmance of KMEANSD++ is comparable to KMEANS++\\n(both of which are better than KMEANSRAND) with\\nKMEANSD++ having an additional that it can automat-\\nically approximate the value of k.\\nIntroduction\\nCLUSTERING is one of the most challenging problems in\\nmachine learning due to the lack of supervision and difﬁ-\\nculty to evaluate its quality. Its aim is to partition the data\\ninto groups, called clusters, such that the members of each\\ngroup are more similar to each other than to the mem-\\nbers of any other group under some measure of similarity,\\ne.g. Euclidean distance. Among many clustering algorithms,\\nKMEANS algorithm (Lloyd 2006), also known as Lloyd’s al-\\ngorithm, is one of the most widely-used, simple and easy to\\nimplement clustering algorithm that works well in practice.\\nHowever, it has no theoretical guarantees in terms of how\\nfar the resulting clustering is from the optimal clustering.\\nArthur and Vassilvitskii (2007) proposed an algorithm called\\nKMEANS++ that samples the initial centroids for the cluster-\\ning algorithm from among the data points in a way that the\\nKMEANS clustering algorithm is able to achieve theoretical\\nguarantees. The underlying idea is that sampling takes into\\naccount the Euclidean distance between points – higher the\\ndistance between a candidate data point from the already se-\\nlected centroids, higher the probability of selecting this data\\npoint as an initial centroid. However, there is one major lim-\\nitation: the number of clusters k needs to be determined by\\nthe user of the algorithm.\\nIn this paper, we consider an alternative sampling scheme\\nto the KMEANS++ algorithm, a new technique of sampling\\nthe initial set of centroids for the KMEANS clustering al-\\ngorithm that overcomes the aforementioned limitation. The\\nnew approach was proposed only recently (Kang 2013) and\\nuses determinantal point processes (DPPs) (Kulesza and\\nTaskar 2012) for sampling. However, the main focus of their\\npaper was to speed up the DPP sampling algorithm. Reichart\\nand Korhonen (2013) use DPPs to cluster verbs with similar\\nsub-categorization frames and selectional preferences. How-\\never, their presentation of the clustering technique is tied\\nto the task and not presented as a general clustering strat-\\negy. Neither of the aforementioned works compare the DPP\\ninitializer with the KMEANS++ initializer and hence do not\\nprovide evidence that one has advantages over the other.\\nThe DPP sampling procedure has a desirable property\\n(for initializing the KMEANS algorithm) that it samples a\\ndiverse sub-set of points (Kulesza and Taskar 2012). In\\nspirit, the notion of diversity in the context of DPPs is sim-\\nilar to the notion of Euclidean distance in the context of\\nKMEANS++ (DPP samples diverse points while KMEANS++\\nsamples points that are far in terms of Euclidean distance).\\nWe explore this conceptual connection between the two\\nsampling techniques and provide empirical evidence that\\nKMEANSD++ is as good as KMEANS++ with additional ad-\\nvantages. In the settings, where KMEANS++ cannot be used,\\nwe compare KMEANSD++ algorithm with the randomly ini-\\ntialized KMEANS algorithm, which we call KMEANSRAND,\\nand show superior performance of the former. We show re-\\nsults on a synthetic data-set and a text clustering task that\\nwas the motivation for us to develop a technique to approxi-\\nmate k for a data-set automatically.\\nRelated work\\nIn this paper we primarily focus on the center-based clus-\\ntering problem where the large dataset can be fairly well\\nrepresented by a small set of cluster centers, e.g. a clus-\\nter center can be a convex combination of the data points\\nin this cluster (we will denote the number of clusters as\\nk) or the most ’representative’ data point from among\\ncluster data points. The most popular clustering algorithm\\nthat is used in this setting is the KMEANS algorithm and\\nits soft version, Expectation-Maximization (EM) (Demp-\\nster, Laird, and Rubin 1977; Liang and Klein 2009). De-\\nspite their simplicty, both algorithms suffer many prob-\\nlems which prevents their usage in practical problems. They\\narXiv:1410.6975v1  [cs.LG]  26 Oct 2014\\nhave no theoretical performance guarantees and the so-\\nlution they recover is extremely sensitive to initialization\\nwhich usually is done uniformly at random (the solution\\nthey converge to can be arbitrarily bad) (von Luxburg 2010;\\nArthur and Vassilvitskii 2007). Also, they may lead to po-\\ntential instability (Bubeck, Meila, and von Luxburg 2012;\\nShamir and Tishby 2008; Rakhlin and Caponnetto 2006;\\nKuncheva and Vetrov 2006).\\nThere only exists few successful attempts to improve\\nthe performance of the KMEANS algorithm in such a way\\nthat the resulting method does have theoretical performance\\nguarantees, meaning it provably approximates a certain mea-\\nsure of clustering quality such as an objective function1.\\nThe most widely-cited objective function used to mea-\\nsure the quality of a center-based clustering is the k-means\\nclustering objective which is computed as the sum of the\\nsquared distances between every data point and its clos-\\nest cluster center. Optimizing this objective is an NP-hard\\nproblem (Aloise et al. 2009) and there only exists a few\\nalgorithms that provably approximate it (Arthur and Vas-\\nsilvitskii 2007; Ailon, Jaiswal, and Monteleoni 2009). The\\nmost popular among them is the KMEANS++ algorithm\\nwhich achieves approximation factor O(log k). Other algo-\\nrithms, this time with constant approximation with respect\\nto the same objective, that were published in the literature\\ninclude (i) the KMEANS# algorithm (Ailon, Jaiswal, and\\nMonteleoni 2009) which, as opposed to the KMEANS++ al-\\ngorithm, returns more than k centers (O(k log k)), (ii) adap-\\ntive sampling-based approach (Aggarwal, Deshpande, and\\nKannan 2009), which returns O(k) centers, (iii) local search\\ntechnique (Kanungo et al. 2002), and (iv) online cluster-\\ning with experts algorithm (Choromanska and Monteleoni\\n2012).\\nOther notable clustering approaches mainly focus on min-\\nimizing other, often less-descriptive to the center-based clus-\\ntering problem, objectives (e.g. k-center or k-medoid objec-\\ntive) (Beygelzimer, Kakade, and Langford 2006; Charikar\\net al. 1997; Guha et al. 2003). Among these techniques\\nalso spectral methods (von Luxburg 2007; Choromanska et\\nal. 2013) are widely-cited however they have a much more\\ngeneral scope than the center-based clustering problem and\\ntherefore will not be discussed in this paper.\\nDeterminantal Point Processes (DPPs)\\nKulesza and Taskar (2012) introduced applications and algo-\\nrithms for using determinantal point processes for machine\\nlearning. Following is a summary of parts of their tutorial\\nrelevant to this paper.\\nA point process is a probability measure P on 2Y, the set\\nof all subsets of Y. This point process is determinantal if the\\nprobability measure satisﬁes the following property: if Y is\\na random subset drawn according to P, then for every subset\\nA ⊆ Y,\\nP(A ⊆ Y) = det(KA)\\n1Standard theoretical guarantees show that the objective func-\\ntion to which the algorithm converges is upper-bounded by the op-\\ntimal value of the objective function multiplied by some bounded\\nsmall constant greater than 1.\\nfor some real N × N matrix K, indexed by the elements\\nof Y. KA ≡ [Kij]i,j∈A denotes the restriction of K to the\\nentries indexed by elements of A, det(KA) stands for the\\ndeterminant of matrix KA, and det (Kφ) = 1. Say, A is a\\nset of two elements, {i, j}. Using the above formula,\\nP(i, j ∈ Y) = KiiKjj − KijKji = KiiKjj − K2\\nij\\nIf the two elements, i, j are similar, then Kij is large,\\nand the probability distribution over the two element set is\\nsmall. Therefore, DPPs, by deﬁnition, put a greater probabil-\\nity mass on sets that have dissimilar elements, as compared\\nto sets that have similar elements. Kulesza and Taskar (2012)\\npresent a sampling algorithm (Algorithm 1, page 16 of their\\ntutorial) for sampling from a DPP. Given a set of points, this\\nalgorithm selects a subset of the most dissimilar points from\\nthe set. In spirit, the notion of diversity in the context of\\nDPPs is similar to the notion of euclidean distance in the\\ncontext of KMEANS++ (DPP samples diverse points while\\nKMEANS++ samples points that are far in terms of euclidean\\ndistance). We explore this conceptual connection between\\nthe two sampling techniques and provide empirical evidence\\nthat KMEANSD++ has advantages over KMEANS++.\\nThe most appealing aspect of the DPP sampling algo-\\nrithm is that it is not required that the number of dissim-\\nilar points be known in advance. Given a set of points,\\nthe DPP sampler returns a subset of dissimilar points. We\\nuse the cardinality of this sampled subset as k for running\\nthe KMEANS clustering algorithm. The DPP sampling algo-\\nrithm, in addition, has a version, called k-DPP (Kulesza and\\nTaskar 2012), in which one may specify k as the cardinal-\\nity of the subset of dissimilar points to be sampled. When\\nwe sample the initial centroids for the KMEANS clustering\\nalgorithm using k-DPP, we refer to the overall scheme as\\nKMEANSDk++.\\nKMEANS++ versus KMEANSD++\\nIn this section we will show the fundamental difference\\nbetween\\nKMEANS++\\nand\\nKMEANSD++\\ninitializers.\\nKMEANS++ algorithm sampling the initial centroids (also\\ncalled seeds) for the KMEANS algorithm is summarized in\\nAlgorithm 1. Here, D(x) denotes the shortest Euclidean\\ndistance from a data point x to the closest seed from among\\nseeds already chosen (S). The\\nKMEANS++ initializer\\nassigns the highest probability to the data point that is\\ncurrently the furthest from its closest seed from among\\nthe set of seeds chosen already. KMEANSD++ initializer\\nchooses the seeds from among the points in the dataset\\nusing different probabilities of selecting a new member for\\nset S. Before showing the algorithm, we will introduce\\nnotation. Let K be the RBF kernel matrix with (i, j)th\\nentrance equal to K(i, j)\\n=\\nexp(−σ∥xi − xj∥2) and\\nσ be a ﬁxed positive constant (note that K is of size\\nn × n and is symmetric positive semi-deﬁnite), KS is a\\nsub-matrix of matrix K of size |S| × |S| deﬁned by points\\nfrom S, and KS∪{x} is a sub-matrix of matrix K of size\\n|S| + 1 × |S| + 1 deﬁned by points from S ∪ {x} (both\\nsub-matrices are as well symmetric positive semi-deﬁnite).\\na)\\n0\\n10\\n20\\n30\\n40\\n0\\n10\\n20\\n30\\n40\\nkmeansD++\\nb)\\n0\\n10\\n20\\n30\\n40\\n0\\n10\\n20\\n30\\n40\\nkmeansDk++\\nc)\\n0\\n10\\n20\\n30\\n40\\n0\\n10\\n20\\n30\\n40\\nkmeans++\\nFigure 1: The seeds recovered by a) KMEANSD++ b) KMEANSDk++ and c) KMEANS++ initializers on a mixture of 25 Gaus-\\nsians.\\nKMEANSD++ algorithm is summarized in Algorithm 2.2\\nAlgorithm 1 KMEANS++\\nInput: dataset X\\n1) S = ∅\\n2) Pick a point uniformly at random from X and add it to S.\\n3) for i = 1 : 1 : k − 1:\\na) choose data point x ∈ X at random with\\nprobability P(x|S) =\\nD(x)2\\nP\\nx′ ∈X D(x′)2\\nb) S = S ∪ {x}\\nAlgorithm 2 KMEANSD++\\nInput: dataset X\\n1, 2 and 4) as in KMEANS++\\n3) for i = 1 : 1 : k − 1:\\nchoose data point x ∈ X at random with\\nprobability P(x|S) = det(KS∪{x})\\ndet(KS)\\nKMEANSD++ favors diversity by putting higher probability\\nto sets of items that are diverse, which is the property that\\nthe KMEANS++ initializer also has, however the former\\nuses less aggressive initialization scheme, i.e. it does not\\nnecessarily put the highest probability to the data point that\\nis currently the furthest from its closest seed from among\\nthe set of seeds chosen already. This can be shown by\\nconsidering a simple example. Let X = {x1, x2, x3} be\\nthe set of points on a 1D line, where x1 was sampled ﬁrst\\nand then x2 and x3. We will consider two possible loca-\\ntions for x3, that we will refer to as x\\n′\\n3 and x\\n′′\\n3, shown below:\\na) x1————–0————–x2————–x\\n′\\n3\\nb) x1————–0————–x2 and x\\n′′\\n3 = 0\\nLet ∥x1∥ = ∥x2∥ = D and ∥x\\n′\\n3 − x2∥ = D − ϵ and\\nlet D be ﬁxed such that D >\\nq\\nlog 6\\n4σ . One can show that\\nthe DPP k-means initializer will put higher probability to\\n2Note that the practical implementation of the KMEANSD++\\nalgorithm differs from the Algorithm 2 and follows Algorithm 1\\n(page 16) from (Kulesza and Taskar 2012), however from the per-\\nspective of the theoretical analysis the simpler version summarized\\nin Algorithm 2 is more convenient.\\nselect x\\n′\\n3 then x\\n′′\\n3, which is captured in Lemma 1. The proof\\nis deferred to the appendix.\\nLemma 1. There exists ϵ ∈ (0, D) such that P(x\\n′\\n3|S) >\\nP(x\\n′′\\n3|S), thus KMEANSD++ initializer can put the highest\\nprobability to the point which is not the furthest from the\\nclosest seed from among seeds already chosen.\\nEvaluation on synthetic datasets\\nkt\\n4\\n9\\n16\\n25\\n36 100\\nTotal\\nKMEANSRAND\\n1\\n3\\n6\\n8\\n14\\n31\\n63\\nKMEANS++\\n0\\n1\\n2\\n2\\n2\\n9\\n16\\nKMEANSDk++\\n0\\n1\\n1\\n2\\n4\\n10\\n18\\nKMEANSD++\\n0\\n0\\n0\\n1\\n0\\n9\\n10\\nk\\n4 11\\n18\\n28\\n39 105\\nTable 1: Comparison of\\nKMEANSRAND,\\nKMEANS++,\\nKMEANSDk++ and KMEANS++ initlializers on synthetic\\ndatasets. Number of clusters missed by each of the three al-\\ngorithms. kt denotes the true number of clusters.\\nWe compare KMEANSRAND, KMEANS++, KMEANSD++\\nand KMEANSDk++ initializers on standard synthetic data-\\nsets. KMEANSDk++ refers to the KMEANSD++ initializer\\nrun with pre-speciﬁed number of clusters (k). For these\\ndatasets we know the true number of clusters, denoted as\\nkt, and we well understand the geometry of the problem.\\nWe use mixture of well-separated Gaussians on a 2D grid.\\nThe variance of each Gaussian is 1, the number of points\\nin each of them is 100 and the separation between them is\\n10. The results are presented in Table 1 (for each experi-\\nment we report the median result over 50 runs). For all the\\nmethods we report the number of missing clusters (missed).\\nFurthermore, for the KMEANSD++ initializer we report the\\nnumber of clusters recovered automatically (k). Addition-\\nally, in Figure 1 we show an exemplary result we obtained\\nfor a mixture of 25 Gaussians. The results indicate that the\\nperformance of KMEANSD++ and KMEANS++ initializers\\nare similar and furthermore KMEANSD++ initializer is able\\nto recover the true number of clusters underlying the data\\nvery accurately without having the number of clusters pre-\\nspeciﬁed (the correlation between kt, the true k and the k\\npredicted by KMEANSD++ is 0.99). This highlights the abil-\\nity of KMEANSD++ to approximate the true k – an ability\\nthat the KMEANS++ initializer does not have.\\nEvaluation on real datasets\\nIn this section, we compare the performance of the KMEANS\\nclustering algorithm initialized in two different ways, us-\\ning the KMEANSD++ initializer and using the KMEANS++.\\nThe comparison is presented on three benchmark datasets:\\niris, ecoli and dermatology.3 The results are averaged over\\n50 runs. Table 2 presents the F1-measures for clustering\\nthe three data-sets using KMEANS++ and KMEANSDk++\\n(KMEANSD++ with the number of clusters pre-speciﬁed).\\nThe results show that the F1-measures (considering the stan-\\ndard deviation) for the two clustering algorithms are compa-\\nrable, which implies that KMEANSD++ is empirically simi-\\nlar to KMEANS++.\\nDatasets\\nkt\\nKMEANS++\\nKMEANSDk++\\niris\\n3\\n0.88±0.08\\n0.87±0.10\\necoli\\n8\\n0.56±0.06\\n0.63±0.06\\ndermatology\\n6\\n0.72±0.12\\n0.68±0.14\\nTable\\n2:\\nF1-measure\\nobtained\\nby\\nKMEANS++\\nand\\nKMEANSDk++ on benchmark datasets.\\nTo highlight that KMEANSD++ is able to automatically\\napproximate the true k while maintaining a good clustering\\nperformance, we compare the value of the KMEANS cluster-\\ning objective (called cost, lower is better) of KMEANSD++\\nand KMEANSDk++. Note, we cannot report F1-measures\\nfor this evaluation since KMEANSD++ automatically selects\\nthe number of clusters, which can be different from the true\\nnumber of clusters.\\nData kt\\nKMEANSD++\\nKMEANSDk++\\nk\\ncost\\ncost\\niris 3 3.80±0.41\\n62.60±9.19\\n92.94±27.16\\necoli 8 6.23±0.89\\n22.42±2.75\\n18.64±1.68\\nderm 6 32.63±0.522122.31±27.363824.52±282.51\\nTable 3: Performance of KMEANSD++ and KMEANSDk++\\non benchmark datasets. kt is the true number of clusters.\\nk is the number of clusters automatically approximated by\\nKMEANSD++. cost is the value of the KMEANS objective.\\nTable 3 shows two results: 1) the k predicted by\\nKMEANSD++ is close the true kt (columns 2 and 3) and 2)\\nthe quality of clustering in terms of the cost of KMEANSD++\\nand KMEANSDk++ is comparable. The exception is the\\ndermatology (derm) dataset, for which interestingly every\\nfeature has 34 attributes which is very close to the num-\\nber of clusters that KMEANSD++ recovered. Since the DPP\\nsampling algorithm uses the eigen-value decomposition, it\\nseems that the sampler is mis-lead in thinking that data-\\nset has ∼\\n34 classes. This behavior of the DPP sam-\\npler is interesting and requires further investigation (per-\\nhaps it is caused by weakly dependent features). Simul-\\ntaneously, the k-means cost of the clusterings recovered\\n3Downloaded from archive.ics.uci.edu/ml/datasets.html.\\nby KMEANSD++ on the dermatology dataset is signiﬁ-\\ncantly lower than the cost of KMEANSDk++. Note that\\nit can be justiﬁed by the fact that when KMEANSD++\\nresp. largely overestimates/underestimates k, the k-means\\ncost of KMEANSD++ should be resp. lower/higher than\\nKMEANSDk++ because choosing resp. larger/smaller k typ-\\nically implies resp. smaller/larger average distance of a data\\npoint to its closest cluster center.\\nEvaluation on a Real Text Clustering Task\\nIn Anonymous 2014, we introduced a novel task of auto-\\nmatically drawing xkcd movie narrative charts (right half\\nof Figure 2) from textual screenplays (top left of Figure 2).\\nWe presented an end-to-end pipeline, employing algorithms\\nfrom natural language processing, social network analysis\\nand machine learning literature. The main focus of Anony-\\nmous 2014 was to present a novel task, its motivation, and\\na basic system pipeline. However, in this paper, we are only\\nconcerned with improving the key component of the pipeline\\n– the text clustering module.\\nWhile for other text clustering tasks, heuristically setting\\nk may not be a major limitation, for the task at hand, it is crit-\\nical that we have an automatic way of selecting (or approxi-\\nmating) k. This is because, in trying to cluster one data-set,\\nit is well justiﬁed to use domain knowledge and human intu-\\nition to set k or to reﬁne k by observing the output. However,\\nfor the task at hand, we need to ﬁnd a clustering per movie.\\nSince there are hundreds of movies, each with unique char-\\nacteristics, heuristically setting k is not feasible.\\nTerminology and Task Deﬁnition\\nTuretsky and Dimitrova (2004) describe the structure of a\\nmovie screenplay. A screenplay is written using a strict for-\\nmatting grammar. It has scene boundaries that textually sep-\\narate scenes of a movie. Figure 2 shows some of the scene\\nboundaries from the movie The Lord of the Rings. A scene\\nboundary indicates whether the scene is to take place inside\\nor outside (INT, EXT), the name of the location, and can po-\\ntentially specify the time of day (e.g. DAY or NIGHT). The\\nclustering task is to cluster scene boundaries (based on their\\nlexical similarity) into k clusters (with k unknown). Since\\nscene boundaries specify the location at which a scene is\\nshot, the goal is to automatically determine the number and\\ndescription of different scene locations in a movie (we re-\\nmove tags INT./EXT., DAY/NIGHT before clustering).\\nScene locations mentioned in scene boundaries are lex-\\nically similar, but not exactly the same. This is because a\\nscene boundary, more often than not, describes a scene lo-\\ncation, along with sub-location(s). For example, in Figure 2,\\nthe scene location Minas Tirith, which is a city, has multiple\\nsub-locations such as “DOCKS” and “HOUSES OF HEAL-\\nING”. Moreover, there are inconsistencies in the scene lo-\\ncation descriptions. For example, some scene location de-\\nscriptions for Pelennor Fields, which is a sub-location as-\\nsociated with Minas Tirith, are present as “PELENNOR\\nFIELDS/MINAS TIRITH”, whereas others are present as\\n“PELENNOR FIELDS”. As a consequence, a simple exact\\nstring matching algorithm is insufﬁcient to ﬁnd scene bound-\\naries that belong to one location.\\nFigure 2: Right half: xkcd movie narrative chart for part of the movie Lord of the Rings. These charts show character interactions.\\nThe horizontal axis is time. The vertical grouping of lines indicates which characters are together at a given time. Source of\\nimage: http://xkcd.com/657/large/ Left top: snippet from the textual screenplay used as input for automatically\\ncreating the chart. Left bottom: scene boundaries and their scene identiﬁer (SID), their cluster identiﬁer (CID), and their plot\\nidentiﬁer (PID).\\nMovie\\n# scenes (n)\\n# locations (gold k)\\nlog(n)\\n√n\\nkKMEANSD++\\nStar Wars\\n137\\n35\\n2.13\\n11.7\\n41.98±3.30\\nThe Last Crusade\\n148\\n57\\n2.17\\n12.16\\n47.72±3.50\\nRaiders of the Lost Ark\\n139\\n73\\n2.14\\n11.78\\n51.56±5.05\\nPirates of the Caribbean\\n140\\n23\\n2.14\\n11.83\\n41.24±4.32\\nThe Bourne Identity\\n160\\n74\\n2.20\\n12.64\\n61.98±5.03\\nBatman\\n209\\n77\\n2.32\\n14.45\\n71.42±5.14\\nCorrelation with gold k\\n0.58\\n1\\n0.59\\n0.58\\n0.84\\nTable 4: List of movies, the number of scene boundaries, and the number of unique locations per movie in our test set (ﬁrst\\nthree columns). Automatically selected k by commonly used heuristic functions (next two columns). Automatically selected\\nk by using DPPs: mean and standard deviation over 50 runs (last two columns). Last row of the table shows the correlation\\nbetween predicted k and gold k.\\nData\\nTo prepare a gold standard for this evaluation, we trained\\ntwo human annotators to read a screenplay and mark all\\nscenes (or scene boundaries) that belong to one location with\\na unique integer (which we refer to as cluster identiﬁer). For\\nexample, in Figure 2, one of our annotators marked scene\\nboundaries (SID) from 131 through 136 with cluster (or lo-\\ncation) identiﬁer (CID) 1. This means that all these scenes\\ntake place at one location, namely MINAS TIRITH. While\\nperforming the annotation task, the annotators used world\\nknowledge that PELENNOR FIELDS is a sub-location of MI-\\nNAS TIRITH and thus should be marked with the same clus-\\nter identiﬁer. Since we are clustering based on lexical simi-\\nlarity, to put lexically dissimilar strings PELENNOR FIELDS\\nand MINAS TIRITH together, our algorithm relies on the fact\\nthat they are mentioned together in a few scene boundaries\\n(as is the case – see scene number 136).\\nAfter a few rounds of training we asked our annotators\\nto fully annotate the screenplay for the movie Pirates of the\\nCaribbean: Dead Man’s Chest. They achieved a high agree-\\nment of 0.86. We then asked our annotators to divide the\\nremaining set of screenplays into half, each responsible for\\none half.\\nTable 4 gives the list of movies we annotated, along with\\nthe number of scenes and number of locations in each movie.\\nWe use these screenplays for evaluating our methodology.\\nEvaluation and Results\\nWe calculate lexical (or string) similarity using a contigu-\\nous word kernel (Lodhi et al. 2002). We compare three ways\\nof sampling the initial centroids for the KMEANS algorithm:\\nKMEANSRAND, KMEANS++, and KMEANSD++.\\nTo set k for KMEANSRAND, we employ common heuris-\\ntics used in the literature: k = log(n) or √n, where n is the\\nnumber of data points. Table 4 presents the predicted number\\nof k using the functions log(n), √n. We run KMEANSD++\\n50 times and report the mean and standard deviation of the\\nnumber of initial centroids selected by the DPP sampling\\nalgorithm automatically. The last row of table 4 shows the\\ncorrelation of the predicted k with the gold k for the three\\nmethods.4 Deciding k using DPPs has a signiﬁcantly higher\\ncorrelation with the gold k (0.84) as compared to other stan-\\ndard methods (0.59 and 0.58). Note that the correlation of\\nthe number of scenes and the gold k is low (0.58), so any\\nmonotonic function of the number of data-points will not\\nhave a much different correlation. This result shows that\\nDPPs are well-suited for choosing k for this data-set.\\nNext, we show that even if we provide the KMEANS al-\\ngorithm with the gold k, sampling using DPPs provides a\\nbetter initialization, which results in a better clustering. Ta-\\nble 5 shows the macro-F1-measures for clustering obtained\\nby three different ways of sampling the initial centroids. The\\nnumbers show that sampling using DPPs results in a signiﬁ-\\ncantly better clustering (higher F1-measure).\\nMovie\\nk\\nKMEANSRAND\\nKMEANS++\\nKMEANSDk++\\nStar Wars 35\\n0.61±0.04\\n0.62±0.02\\n0.63±0.04\\nCrusade\\n57\\n0.80±0.04\\n0.84±0.02\\n0.86±0.02\\nRaiders\\n73\\n0.68±0.03\\n0.76± 0.02\\n0.77± 0.02\\nPirates\\n23\\n0.62±0.04\\n0.63±0.02\\n0.61±0.04\\nBourne\\n74\\n0.64±0.03\\n0.69±0.03\\n0.68±0.05\\nBatman\\n77\\n0.62±0.03\\n0.63±0.02\\n0.66±0.03\\nTable 5: Mean and standard deviations of F1-measure on the\\ntest set.\\nConclusion and Future Work\\nWe conclude that KMEANSD++ compares favorably to\\nKMEANS++ and performs better than KMEANSRAND with\\ntwo additional advantages: it may be used in scenarios where\\nexplicit feature representation is absent and where the k is\\nunknown. In the future, we will attempt to prove approx-\\nimation guarantees with respect to the k-means clustering\\nobjective for the KMEANSD++ algorithm.\\nAppendix\\nFirst, we will show a useful lemma that we will use later.\\nLemma 2. There exists ϵ ∈ (0, D) such that\\nexp(−2γD2(1− ϵ\\nD)2)−exp(−2γD2)< exp(−2γD2)\\n2\\n(1)\\nProof. For a ﬁxed D this result is straight-forward.\\nProof of Lemma 1. x1, x2 and x3 are respectively the ﬁrst,\\nsecond and third data point chosen by the KMEANSD++ ini-\\ntializer. Thus we have that\\nP(x2|S = x1) = det(Kx2∪x1)\\ndet(Kx1)\\n= 1−exp(−2σ∥x1−x2∥2)\\nand\\nP(x3|S = x1 ∪ x2) = det(Kx1∪x2∪x3)\\ndet(Kx1∪x2)\\n= 1 −\\n1\\n1 − exp(−2σ∥x1 − x2∥2) · {exp(−2σ∥x2 − x3∥2)\\n4Multiplying or adding a constant to the functions log(n), √n\\nwill not change the correlation.\\n+ exp(−2σ∥x1 − x3∥2)\\n−2 exp(−σ(∥x1 − x2∥2 + ∥x2 − x3∥2 + ∥x1 − x3∥2))}\\nNote that since ∥x1 − x2∥ = 2D, ∥x\\n′\\n3 − x2∥ = D − ϵ,\\n∥x\\n′\\n3 − x1∥ = 3D − ϵ, ∥x\\n′′\\n3 − x2∥ = D and ∥x\\n′′\\n3 − x1∥ = D,\\nthe following chain of inequlities are equivalent:\\nP(x\\n′\\n3|S) > P(x\\n′′\\n3|S)\\n⇐⇒ exp(−2σ(3D − ϵ)2) + exp(−2σ(D − ϵ)2)\\n−2 exp(−σ(4D2 + (3D − ϵ)2 + (D − ϵ)2))\\n≤ 2 exp(−2σD2) − 2 exp(−6σD2)\\n⇐⇒ exp(−18σD2(1 −\\nϵ\\n3D)2) + exp(−2σD2(1 − ϵ\\nD)2)\\n−2 exp(−σ(4D2 + (3D − ϵ)2 + (D − ϵ)2))\\n≤ 2 exp(−2σD2) − 2 exp(−6σD2)\\nWe want to prove that the last inequality holds. We will show\\nthat by instead showing the series of stronger inequalities\\nthat hold and imply the above one. Note, that the inequality\\nthat implies the above one is given below\\nexp(−18σD2(1 −\\nϵ\\n3D)2) + exp(−2σD2(1 − ϵ\\nD)2)\\n≤ 2 exp(−2σD2) − 2 exp(−6σD2)\\n(2)\\nThis inequality can be rewritten as\\n2 exp(−6σD2) + exp(−18σD2(1 −\\nϵ\\n3D)2)\\n+ exp(−2σD2(1 − ϵ\\nD)2) ≤ 2 exp(−2σD2)\\nRecall that ϵ < D thus\\n1− ϵ\\n3D > 2\\n3 ⇐⇒ exp(−18σD2(1− ϵ\\n3D)2) < exp(−8σD2)\\nThus, an even stronger inequality than the one in Equation 2\\nis the following one\\n2 exp(−6σD2) + exp(−8σD2)\\n+ exp(−2σD2(1 − ϵ\\nD)2) < 2 exp(−2σD2)\\n(3)\\nThe inequality in Equation 3 implies the inequality in Equa-\\ntion 2. Note that exp(−8σD2) ≤ exp(−6σD2) thus one can\\nconstruct an even stronger inequality given in Equation 4,\\nthan the one in Equation 3 that directly implies Equation 3\\nand therefore also Equation 2.\\n3 exp(−6σD2)+exp(−2σD2(1−ϵ\\nD)2)≤2 exp(−2σD2) (4)\\nRecall that\\nD >\\nr\\nlog 6\\n4σ\\n⇐⇒ 3 exp(−6σD2) < 1\\n2 exp(−2σD2)\\nFinally, we will below provide the last inequality, in Equa-\\ntion 5, which is the strongest from all discussed before as, if\\nit holds, it directly implies the inequalities in Equation 4 and\\ntherefore also Equation 3 and 2.\\nexp(−2σD2)\\n2\\n+ exp(−2σD2(1 − ϵ\\nD)2) < 2 exp(−2σD2)\\n(5)\\nThis equality can be equivalently rewritten as\\nexp(−2σD2(1 − ϵ\\nD)2) − exp(−2σD2) < exp(−2σD2)\\n2\\n,\\nwhere the last inequality holds by Lemma 2.\\nReferences\\n[Aggarwal, Deshpande, and Kannan 2009] Aggarwal,\\nA.;\\nDeshpande, A.; and Kannan, R. 2009. Adaptive sampling\\nfor k-means clustering. In APPROX.\\n[Ailon, Jaiswal, and Monteleoni 2009] Ailon, N.; Jaiswal,\\nR.; and Monteleoni, C. 2009. Streaming k-means approxi-\\nmation. In NIPS.\\n[Aloise et al. 2009] Aloise, D.; Deshpande, A.; Hansen, P.;\\nand Popat, P.\\n2009.\\nNp-hardness of euclidean sum-of-\\nsquares clustering. Machine Learning 75(2):245–248.\\n[Arthur and Vassilvitskii 2007] Arthur, D., and Vassilvitskii,\\nS. 2007. k-means++: the advantages of careful seeding. In\\nSODA.\\n[Beygelzimer, Kakade, and Langford 2006] Beygelzimer,\\nA.; Kakade, S.; and Langford, J.\\n2006.\\nCover trees for\\nnearest neighbor. In ICML.\\n[Bubeck, Meila, and von Luxburg 2012] Bubeck, S.; Meila,\\nM.; and von Luxburg, U. 2012. How the initialization affects\\nthe stability of the k-means algorithm. ESAIM: Probability\\nand Statistics 16:436–452.\\n[Charikar et al. 1997] Charikar, M.; Chekuri, C.; Feder, T.;\\nand Motwani, R. 1997. Incremental clustering and dynamic\\ninformation retrieval. In STOC.\\n[Choromanska and Monteleoni 2012] Choromanska, A., and\\nMonteleoni, C. 2012. Online clustering with experts. In\\nAISTATS.\\n[Choromanska et al. 2013] Choromanska, A.; Jebara, T.;\\nKim, H.; Mohan, M.; and Monteleoni, C. 2013. Fast spectral\\nclustering via the nystrÃ˝um method. In ALT.\\n[Dempster, Laird, and Rubin 1977] Dempster, A. P.; Laird,\\nN. M.; and Rubin, D. B. 1977. Maximum likelihood from\\nincomplete data via the em algorithm. Journal of the Royal\\nStatistical Society, Series B 39(1):1–38.\\n[Guha et al. 2003] Guha, S.; Meyerson, A.; Mishra, N.; Mot-\\nwani, R.; and O’Callaghan, L.\\n2003.\\nClustering data\\nstreams: Theory and practice. IEEE Trans. on Knowl. and\\nData Eng. 15(3):515–528.\\n[Kang 2013] Kang, B. 2013. Fast determinantal point pro-\\ncess sampling with application to clustering. In NIPS.\\n[Kanungo et al. 2002] Kanungo, T.; Mount, D. M.; Ne-\\ntanyahu, N. S.; Piatko, C. D.; Silverman, R.; and Wu, A. Y.\\n2002. A local search approximation algorithm for k-means\\nclustering. In Symposium on Computational Geometry, 10–\\n18.\\n[Kulesza and Taskar 2012] Kulesza,\\nA.,\\nand\\nTaskar,\\nB.\\n2012. Determinantal point processes for machine learning.\\narXiv:1207.6083.\\n[Kuncheva and Vetrov 2006] Kuncheva, L. I., and Vetrov,\\nD. P.\\n2006.\\nEvaluation of stability of k-means clus-\\nter ensembles with respect to random initialization. IEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\n28(11):1798–1808.\\n[Liang and Klein 2009] Liang, P., and Klein, D. 2009. On-\\nline em for unsupervised models. In NAACL.\\n[Lloyd 2006] Lloyd, S. 2006. Least squares quantization in\\npcm. IEEE Trans. Inf. Theor. 28(2):129–137.\\n[Lodhi et al. 2002] Lodhi, H.; Saunders, C.; Shawe-Taylor,\\nJ.; Christianini, N.; and Watkins, C. 2002. Text classiﬁca-\\ntion using string kernels. The Journal of Machine Learning\\nResearch 2:419–444.\\n[Rakhlin and Caponnetto 2006] Rakhlin, A., and Capon-\\nnetto, A. 2006. Stability of k-means clustering. In NIPS.\\n[Reichart and Korhonen 2013] Reichart, R., and Korhonen,\\nA. 2013. Improved lexical acquisition through dpp-based\\nverb clustering. In Proceedings of the 51st Annual Meet-\\ning of the Association for Computational Linguistics (Vol-\\nume 1: Long Papers), 862–872. Soﬁa, Bulgaria: Association\\nfor Computational Linguistics.\\n[Shamir and Tishby 2008] Shamir, O., and Tishby, N. 2008.\\nModel selection and stability in k-means clustering.\\nIn\\nCOLT.\\n[Turetsky and Dimitrova 2004] Turetsky, R., and Dimitrova,\\nN. 2004. Screenplay alignment for closed-system speaker\\nidentiﬁcation and analysis of feature ﬁlms. In Multimedia\\nand Expo, 2004. ICME’04. 2004 IEEE International Con-\\nference on, volume 3, 1659–1662.\\n[von Luxburg 2007] von Luxburg, U. 2007. A tutorial on\\nspectral clustering.\\nStatistics and Computing 17(4):395–\\n416.\\n[von Luxburg 2010] von Luxburg, U. 2010. Clustering sta-\\nbility: An overview. Found. Trends Mach. Learn. 2(3):235–\\n274.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.vectorstores import Chroma\n",
        "# from langchain.embeddings import OpenAIEmbeddings\n",
        "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=500)\n",
        "\n",
        "# docs = text_splitter.split_documents(base_docs)\n",
        "# vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())"
      ],
      "metadata": {
        "id": "d0mGsBMaTCgd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Indexing\n",
        "1.   Split each paper text into chunks and convert them into embeddings\n",
        "2.   Store embeddings into vectore database (FAISS)"
      ],
      "metadata": {
        "id": "1BI88Vhh-lFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "KET3jKgOe05g"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500)\n",
        "\n",
        "docs = text_splitter.split_documents(base_docs)\n",
        "# vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())\n",
        "\n",
        "vectorstore = FAISS.from_documents(docs, OpenAIEmbeddings())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCeqKxbRe4ko",
        "outputId": "4d39491d-35a7-4ee2-c974-ddf4be4f3888"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)\n",
        "print(max([len(chunk.page_content) for chunk in docs]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkZ-RhxstM8O",
        "outputId": "b99ab408-bed9-4fd9-c32d-6519e7e7d0f5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "499\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieving relevant documents. We use top k method to retrieve N relevant papers and \"mmr\" (Maximal Marginal Relevance) to select different results"
      ],
      "metadata": {
        "id": "41u8RsL1_LzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":5})\n",
        "relevant_docs = base_retriever.get_relevant_documents( \"k-Means cluster algorithm\")\n",
        "len(relevant_docs)\n",
        "for doc in relevant_docs:\n",
        "  print(doc.metadata)"
      ],
      "metadata": {
        "id": "R-0LCjUaT0ql",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bbd815d-eeea-43d0-fb4f-883d7cf4beea"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Published': '2022-05-09', 'Title': 'A Hybrid Approach: Utilising Kmeans Clustering and Naive Bayes for IoT Anomaly Detection', 'Authors': 'Lincoln Best, Ernest Foo, Hui Tian', 'Summary': 'The proliferation and variety of Internet of Things devices means that they\\nhave increasingly become a viable target for malicious users. This has created\\na need for anomaly detection algorithms that can work across multiple devices.\\nThis thesis suggests a potential alternative to the current anomaly detection\\nalgorithms to be implemented within IoT systems that can be applied across\\ndifferent types of devices. This algorithm is comprised of both unsupverised\\nand supervised machine areas of machine learning combining the strongest facet\\nof each. The algorithm involves the initial k-means clustering of attacks and\\nassigns them to clusters. Next, the clusters are then used by the AdaBoosted\\nNaive Bayes supervised learning algorithm in order to teach itself which piece\\nof data should be clustered to which specific attack. This increases the\\naccuracy of the proposed algorithm by adding clustered data before the final\\nclassification step, ensuring a more accurate algorithm. The correct\\nindentification percentage scores for this proposed algorithm range anywhere\\nfrom 90% to 100%, as well as rating the proposed algorithms accuracy, precision\\nand recall. These high scores achieve an accurate, flexible, scalable,\\noptimised algorithm that could potentially be in different IoT devices,\\nensuring strong data integrity and privacy.'}\n",
            "{'Published': '2014-10-26', 'Title': 'Notes on using Determinantal Point Processes for Clustering with Applications to Text Clustering', 'Authors': 'Apoorv Agarwal, Anna Choromanska, Krzysztof Choromanski', 'Summary': 'In this paper, we compare three initialization schemes for the KMEANS\\nclustering algorithm: 1) random initialization (KMEANSRAND), 2) KMEANS++, and\\n3) KMEANSD++. Both KMEANSRAND and KMEANS++ have a major that the value of k\\nneeds to be set by the user of the algorithms. (Kang 2013) recently proposed a\\nnovel use of determinantal point processes for sampling the initial centroids\\nfor the KMEANS algorithm (we call it KMEANSD++). They, however, do not provide\\nany evaluation establishing that KMEANSD++ is better than other algorithms. In\\nthis paper, we show that the performance of KMEANSD++ is comparable to KMEANS++\\n(both of which are better than KMEANSRAND) with KMEANSD++ having an additional\\nthat it can automatically approximate the value of k.'}\n",
            "{'Published': '2024-02-19', 'Title': 'Kernel KMeans clustering splits for end-to-end unsupervised decision trees', 'Authors': 'Louis Ohl, Pierre-Alexandre Mattei, Mickaël Leclercq, Arnaud Droit, Frédéric Precioso', 'Summary': 'Trees are convenient models for obtaining explainable predictions on\\nrelatively small datasets. Although there are many proposals for the end-to-end\\nconstruction of such trees in supervised learning, learning a tree end-to-end\\nfor clustering without labels remains an open challenge. As most works focus on\\ninterpreting with trees the result of another clustering algorithm, we present\\nhere a novel end-to-end trained unsupervised binary tree for clustering: Kauri.\\nThis method performs a greedy maximisation of the kernel KMeans objective\\nwithout requiring the definition of centroids. We compare this model on\\nmultiple datasets with recent unsupervised trees and show that Kauri performs\\nidentically when using a linear kernel. For other kernels, Kauri often\\noutperforms the concatenation of kernel KMeans and a CART decision tree.'}\n",
            "{'Published': '2019-10-15', 'Title': 'MSD-Kmeans: A Novel Algorithm for Efficient Detection of Global and Local Outliers', 'Authors': 'Yuanyuan Wei, Julian Jang-Jaccard, Fariza Sabrina, Timothy McIntosh', 'Summary': 'Outlier detection is a technique in data mining that aims to detect unusual\\nor unexpected records in the dataset. Existing outlier detection algorithms\\nhave different pros and cons and exhibit different sensitivity to noisy data\\nsuch as extreme values. In this paper, we propose a novel cluster-based outlier\\ndetection algorithm named MSD-Kmeans that combines the statistical method of\\nMean and Standard Deviation (MSD) and the machine learning clustering algorithm\\nK-means to detect outliers more accurately with the better control of extreme\\nvalues. There are two phases in this combination method of MSD-Kmeans: (1)\\napplying MSD algorithm to eliminate as many noisy data to minimize the\\ninterference on clusters, and (2) applying K-means algorithm to obtain local\\noptimal clusters. We evaluate our algorithm and demonstrate its effectiveness\\nin the context of detecting possible overcharging of taxi fares, as greedy\\ndishonest drivers may attempt to charge high fares by detouring. We compare the\\nperformance indicators of MSD-Kmeans with those of other outlier detection\\nalgorithms, such as MSD, K-means, Z-score, MIQR and LOF, and prove that the\\nproposed MSD-Kmeans algorithm achieves the highest measure of precision,\\naccuracy, and F-measure. We conclude that MSD-Kmeans can be used for effective\\nand efficient outlier detection on data of varying quality on IoT devices.'}\n",
            "{'Published': '2023-05-01', 'Title': 'CKmeans and FCKmeans : Two deterministic initialization procedures for Kmeans algorithm using a modified crowding distance', 'Authors': 'Abdesslem Layeb', 'Summary': 'This paper presents two novel deterministic initialization procedures for\\nK-means clustering based on a modified crowding distance. The procedures, named\\nCKmeans and FCKmeans, use more crowded points as initial centroids.\\nExperimental studies on multiple datasets demonstrate that the proposed\\napproach outperforms Kmeans and Kmeans++ in terms of clustering accuracy. The\\neffectiveness of CKmeans and FCKmeans is attributed to their ability to select\\nbetter initial centroids based on the modified crowding distance. Overall, the\\nproposed approach provides a promising alternative for improving K-means\\nclustering.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create prompt and response templates"
      ],
      "metadata": {
        "id": "egI3uXoc_uvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough"
      ],
      "metadata": {
        "id": "Dqv3TiZww0LF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Answer the question based only on the following context. If you cannot answer the question with the context, please respond with 'I don't know':\n",
        "\n",
        "### CONTEXT\n",
        "{context}\n",
        "\n",
        "### QUESTION\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "def generate_summary_for_doc(doc, language_model, template):\n",
        "    # Extracting information from the Document object\n",
        "    title = doc.metadata.get('Title', 'No Title')\n",
        "    authors = doc.metadata.get('Authors', 'Unknown Authors')\n",
        "    published = doc.metadata.get('Published', 'Unknown Date')\n",
        "    page_content = doc.page_content\n",
        "\n",
        "    # Forming the context with the document information\n",
        "    context = f\"**Title**: {title}\\n**Authors**: {authors}\\n**Published**: {published}\\n**Content**: {page_content}\"\n",
        "\n",
        "    # Forming the full prompt by substituting context into the template\n",
        "    full_prompt = template.format(context=context, question=\"Summarize the content in no more than two lines.\")\n",
        "\n",
        "    # Sending the prompt to the language model for generating the summary\n",
        "    summary = language_model.invoke(full_prompt)\n",
        "\n",
        "    # Updating the document's metadata with the summary\n",
        "    doc.metadata['LLM_Summary'] = summary\n",
        "\n",
        "    return doc.metadata\n",
        "\n",
        "# Initialize the language model\n",
        "primary_qa_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Zlho58ERss2",
        "outputId": "ae8278c9-f5e9-4a1c-9f71-bd8925740f5f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Published': '2022-05-09', 'Title': 'A Hybrid Approach: Utilising Kmeans Clustering and Naive Bayes for IoT Anomaly Detection', 'Authors': 'Lincoln Best, Ernest Foo, Hui Tian', 'Summary': 'The proliferation and variety of Internet of Things devices means that they\\nhave increasingly become a viable target for malicious users. This has created\\na need for anomaly detection algorithms that can work across multiple devices.\\nThis thesis suggests a potential alternative to the current anomaly detection\\nalgorithms to be implemented within IoT systems that can be applied across\\ndifferent types of devices. This algorithm is comprised of both unsupverised\\nand supervised machine areas of machine learning combining the strongest facet\\nof each. The algorithm involves the initial k-means clustering of attacks and\\nassigns them to clusters. Next, the clusters are then used by the AdaBoosted\\nNaive Bayes supervised learning algorithm in order to teach itself which piece\\nof data should be clustered to which specific attack. This increases the\\naccuracy of the proposed algorithm by adding clustered data before the final\\nclassification step, ensuring a more accurate algorithm. The correct\\nindentification percentage scores for this proposed algorithm range anywhere\\nfrom 90% to 100%, as well as rating the proposed algorithms accuracy, precision\\nand recall. These high scores achieve an accurate, flexible, scalable,\\noptimised algorithm that could potentially be in different IoT devices,\\nensuring strong data integrity and privacy.', 'LLM_Summary': AIMessage(content='Answer: The content discusses the use of Kmeans clustering and Naive Bayes for IoT anomaly detection, explaining the iterative process of Kmeans clustering in detail.')}\n",
            "{'Published': '2014-10-26', 'Title': 'Notes on using Determinantal Point Processes for Clustering with Applications to Text Clustering', 'Authors': 'Apoorv Agarwal, Anna Choromanska, Krzysztof Choromanski', 'Summary': 'In this paper, we compare three initialization schemes for the KMEANS\\nclustering algorithm: 1) random initialization (KMEANSRAND), 2) KMEANS++, and\\n3) KMEANSD++. Both KMEANSRAND and KMEANS++ have a major that the value of k\\nneeds to be set by the user of the algorithms. (Kang 2013) recently proposed a\\nnovel use of determinantal point processes for sampling the initial centroids\\nfor the KMEANS algorithm (we call it KMEANSD++). They, however, do not provide\\nany evaluation establishing that KMEANSD++ is better than other algorithms. In\\nthis paper, we show that the performance of KMEANSD++ is comparable to KMEANS++\\n(both of which are better than KMEANSRAND) with KMEANSD++ having an additional\\nthat it can automatically approximate the value of k.', 'LLM_Summary': AIMessage(content='Answer: The content discusses the performance of different clustering algorithms, specifically KMEANSD++, KMEANS++, and KMEANSRAND, in the context of text clustering.')}\n",
            "{'Published': '2024-02-19', 'Title': 'Kernel KMeans clustering splits for end-to-end unsupervised decision trees', 'Authors': 'Louis Ohl, Pierre-Alexandre Mattei, Mickaël Leclercq, Arnaud Droit, Frédéric Precioso', 'Summary': 'Trees are convenient models for obtaining explainable predictions on\\nrelatively small datasets. Although there are many proposals for the end-to-end\\nconstruction of such trees in supervised learning, learning a tree end-to-end\\nfor clustering without labels remains an open challenge. As most works focus on\\ninterpreting with trees the result of another clustering algorithm, we present\\nhere a novel end-to-end trained unsupervised binary tree for clustering: Kauri.\\nThis method performs a greedy maximisation of the kernel KMeans objective\\nwithout requiring the definition of centroids. We compare this model on\\nmultiple datasets with recent unsupervised trees and show that Kauri performs\\nidentically when using a linear kernel. For other kernels, Kauri often\\noutperforms the concatenation of kernel KMeans and a CART decision tree.', 'LLM_Summary': AIMessage(content='The authors propose an end-to-end unsupervised decision tree method using Kernel KMeans clustering splits. They define an objective function to minimize the cluster sum of squares in a Hilbert space with respect to centroids.')}\n",
            "{'Published': '2019-10-15', 'Title': 'MSD-Kmeans: A Novel Algorithm for Efficient Detection of Global and Local Outliers', 'Authors': 'Yuanyuan Wei, Julian Jang-Jaccard, Fariza Sabrina, Timothy McIntosh', 'Summary': 'Outlier detection is a technique in data mining that aims to detect unusual\\nor unexpected records in the dataset. Existing outlier detection algorithms\\nhave different pros and cons and exhibit different sensitivity to noisy data\\nsuch as extreme values. In this paper, we propose a novel cluster-based outlier\\ndetection algorithm named MSD-Kmeans that combines the statistical method of\\nMean and Standard Deviation (MSD) and the machine learning clustering algorithm\\nK-means to detect outliers more accurately with the better control of extreme\\nvalues. There are two phases in this combination method of MSD-Kmeans: (1)\\napplying MSD algorithm to eliminate as many noisy data to minimize the\\ninterference on clusters, and (2) applying K-means algorithm to obtain local\\noptimal clusters. We evaluate our algorithm and demonstrate its effectiveness\\nin the context of detecting possible overcharging of taxi fares, as greedy\\ndishonest drivers may attempt to charge high fares by detouring. We compare the\\nperformance indicators of MSD-Kmeans with those of other outlier detection\\nalgorithms, such as MSD, K-means, Z-score, MIQR and LOF, and prove that the\\nproposed MSD-Kmeans algorithm achieves the highest measure of precision,\\naccuracy, and F-measure. We conclude that MSD-Kmeans can be used for effective\\nand efficient outlier detection on data of varying quality on IoT devices.', 'LLM_Summary': AIMessage(content='Answer: The MSD-Kmeans algorithm improves the efficiency and accuracy of K-means by using a statistical algorithm to eliminate outliers and then partitioning the remaining data into clusters using K-means.')}\n",
            "{'Published': '2023-05-01', 'Title': 'CKmeans and FCKmeans : Two deterministic initialization procedures for Kmeans algorithm using a modified crowding distance', 'Authors': 'Abdesslem Layeb', 'Summary': 'This paper presents two novel deterministic initialization procedures for\\nK-means clustering based on a modified crowding distance. The procedures, named\\nCKmeans and FCKmeans, use more crowded points as initial centroids.\\nExperimental studies on multiple datasets demonstrate that the proposed\\napproach outperforms Kmeans and Kmeans++ in terms of clustering accuracy. The\\neffectiveness of CKmeans and FCKmeans is attributed to their ability to select\\nbetter initial centroids based on the modified crowding distance. Overall, the\\nproposed approach provides a promising alternative for improving K-means\\nclustering.', 'LLM_Summary': AIMessage(content='The content discusses the popular initialization method k-means++ for the K-means clustering algorithm, which selects centroids based on distance from existing centroids to improve clustering results.')}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test RAG app: For each paper we retrieved we can see Title, Authors, Published, Summary (Paper's Abstract) and LLM generated Summary in the field \"LLM Summary\" based on question above"
      ],
      "metadata": {
        "id": "NAl4bnuJAFxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "for doc in relevant_docs:\n",
        "  summary_output = generate_summary_for_doc(doc, primary_qa_llm, template)\n",
        "  print(summary_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVvW0-E_ADOm",
        "outputId": "4d62f4b9-51aa-45d6-ac71-5d251d11791b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Published': '2022-05-09', 'Title': 'A Hybrid Approach: Utilising Kmeans Clustering and Naive Bayes for IoT Anomaly Detection', 'Authors': 'Lincoln Best, Ernest Foo, Hui Tian', 'Summary': 'The proliferation and variety of Internet of Things devices means that they\\nhave increasingly become a viable target for malicious users. This has created\\na need for anomaly detection algorithms that can work across multiple devices.\\nThis thesis suggests a potential alternative to the current anomaly detection\\nalgorithms to be implemented within IoT systems that can be applied across\\ndifferent types of devices. This algorithm is comprised of both unsupverised\\nand supervised machine areas of machine learning combining the strongest facet\\nof each. The algorithm involves the initial k-means clustering of attacks and\\nassigns them to clusters. Next, the clusters are then used by the AdaBoosted\\nNaive Bayes supervised learning algorithm in order to teach itself which piece\\nof data should be clustered to which specific attack. This increases the\\naccuracy of the proposed algorithm by adding clustered data before the final\\nclassification step, ensuring a more accurate algorithm. The correct\\nindentification percentage scores for this proposed algorithm range anywhere\\nfrom 90% to 100%, as well as rating the proposed algorithms accuracy, precision\\nand recall. These high scores achieve an accurate, flexible, scalable,\\noptimised algorithm that could potentially be in different IoT devices,\\nensuring strong data integrity and privacy.', 'LLM_Summary': AIMessage(content='Answer: The content discusses the use of Kmeans clustering and Naive Bayes for IoT anomaly detection, explaining the iterative process of Kmeans clustering in detail.')}\n",
            "{'Published': '2014-10-26', 'Title': 'Notes on using Determinantal Point Processes for Clustering with Applications to Text Clustering', 'Authors': 'Apoorv Agarwal, Anna Choromanska, Krzysztof Choromanski', 'Summary': 'In this paper, we compare three initialization schemes for the KMEANS\\nclustering algorithm: 1) random initialization (KMEANSRAND), 2) KMEANS++, and\\n3) KMEANSD++. Both KMEANSRAND and KMEANS++ have a major that the value of k\\nneeds to be set by the user of the algorithms. (Kang 2013) recently proposed a\\nnovel use of determinantal point processes for sampling the initial centroids\\nfor the KMEANS algorithm (we call it KMEANSD++). They, however, do not provide\\nany evaluation establishing that KMEANSD++ is better than other algorithms. In\\nthis paper, we show that the performance of KMEANSD++ is comparable to KMEANS++\\n(both of which are better than KMEANSRAND) with KMEANSD++ having an additional\\nthat it can automatically approximate the value of k.', 'LLM_Summary': AIMessage(content='Answer: The paper discusses the performance of different clustering algorithms, specifically KMEANSD++, KMEANS++, and KMEANSRAND, with a focus on text clustering.')}\n",
            "{'Published': '2024-02-19', 'Title': 'Kernel KMeans clustering splits for end-to-end unsupervised decision trees', 'Authors': 'Louis Ohl, Pierre-Alexandre Mattei, Mickaël Leclercq, Arnaud Droit, Frédéric Precioso', 'Summary': 'Trees are convenient models for obtaining explainable predictions on\\nrelatively small datasets. Although there are many proposals for the end-to-end\\nconstruction of such trees in supervised learning, learning a tree end-to-end\\nfor clustering without labels remains an open challenge. As most works focus on\\ninterpreting with trees the result of another clustering algorithm, we present\\nhere a novel end-to-end trained unsupervised binary tree for clustering: Kauri.\\nThis method performs a greedy maximisation of the kernel KMeans objective\\nwithout requiring the definition of centroids. We compare this model on\\nmultiple datasets with recent unsupervised trees and show that Kauri performs\\nidentically when using a linear kernel. For other kernels, Kauri often\\noutperforms the concatenation of kernel KMeans and a CART decision tree.', 'LLM_Summary': AIMessage(content='The authors propose a Kernel KMeans clustering algorithm for unsupervised decision trees, aiming to minimize the cluster sum of squares in a Hilbert space with Kmax centroids.')}\n",
            "{'Published': '2019-10-15', 'Title': 'MSD-Kmeans: A Novel Algorithm for Efficient Detection of Global and Local Outliers', 'Authors': 'Yuanyuan Wei, Julian Jang-Jaccard, Fariza Sabrina, Timothy McIntosh', 'Summary': 'Outlier detection is a technique in data mining that aims to detect unusual\\nor unexpected records in the dataset. Existing outlier detection algorithms\\nhave different pros and cons and exhibit different sensitivity to noisy data\\nsuch as extreme values. In this paper, we propose a novel cluster-based outlier\\ndetection algorithm named MSD-Kmeans that combines the statistical method of\\nMean and Standard Deviation (MSD) and the machine learning clustering algorithm\\nK-means to detect outliers more accurately with the better control of extreme\\nvalues. There are two phases in this combination method of MSD-Kmeans: (1)\\napplying MSD algorithm to eliminate as many noisy data to minimize the\\ninterference on clusters, and (2) applying K-means algorithm to obtain local\\noptimal clusters. We evaluate our algorithm and demonstrate its effectiveness\\nin the context of detecting possible overcharging of taxi fares, as greedy\\ndishonest drivers may attempt to charge high fares by detouring. We compare the\\nperformance indicators of MSD-Kmeans with those of other outlier detection\\nalgorithms, such as MSD, K-means, Z-score, MIQR and LOF, and prove that the\\nproposed MSD-Kmeans algorithm achieves the highest measure of precision,\\naccuracy, and F-measure. We conclude that MSD-Kmeans can be used for effective\\nand efficient outlier detection on data of varying quality on IoT devices.', 'LLM_Summary': AIMessage(content='Answer: The MSD-Kmeans algorithm improves the efficiency and accuracy of K-means by using a statistical algorithm to eliminate outliers and then partitioning the remaining data into clusters using K-means.')}\n",
            "{'Published': '2023-05-01', 'Title': 'CKmeans and FCKmeans : Two deterministic initialization procedures for Kmeans algorithm using a modified crowding distance', 'Authors': 'Abdesslem Layeb', 'Summary': 'This paper presents two novel deterministic initialization procedures for\\nK-means clustering based on a modified crowding distance. The procedures, named\\nCKmeans and FCKmeans, use more crowded points as initial centroids.\\nExperimental studies on multiple datasets demonstrate that the proposed\\napproach outperforms Kmeans and Kmeans++ in terms of clustering accuracy. The\\neffectiveness of CKmeans and FCKmeans is attributed to their ability to select\\nbetter initial centroids based on the modified crowding distance. Overall, the\\nproposed approach provides a promising alternative for improving K-means\\nclustering.', 'LLM_Summary': AIMessage(content='The content discusses the popular initialization method k-means++ for the K-means clustering algorithm, which selects centroids based on distance from existing centroids to improve clustering results.')}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in relevant_docs:\n",
        "  summary_output = generate_summary_for_doc(doc, primary_qa_llm, template)\n",
        "  print(\"Title:\", summary_output['Title'])\n",
        "  print(\"LLM generated summary:\", summary_output['LLM_Summary'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gv8bTRwrtpgD",
        "outputId": "bdb82104-872e-43ae-a9f8-844131a1e682"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: A Hybrid Approach: Utilising Kmeans Clustering and Naive Bayes for IoT Anomaly Detection\n",
            "LLM generated summary: content='The content discusses the use of Kmeans clustering and Naive Bayes for IoT anomaly detection, explaining the iterative process of Kmeans clustering in three steps.'\n",
            "Title: Notes on using Determinantal Point Processes for Clustering with Applications to Text Clustering\n",
            "LLM generated summary: content='Answer: The content discusses the performance of different clustering algorithms, specifically KMEANSD++, KMEANS++, and KMEANSRAND, in the context of text clustering.'\n",
            "Title: Kernel KMeans clustering splits for end-to-end unsupervised decision trees\n",
            "LLM generated summary: content='The authors propose a Kernel KMeans clustering algorithm for unsupervised decision trees, aiming to minimize the cluster sum of squares in a Hilbert space with Kmax centroids.'\n",
            "Title: MSD-Kmeans: A Novel Algorithm for Efficient Detection of Global and Local Outliers\n",
            "LLM generated summary: content='The MSD-Kmeans algorithm improves the efficiency and accuracy of K-means by using a statistical algorithm to eliminate outliers in the first phase, followed by partitioning the remaining data into clusters using K-means in the second phase.'\n",
            "Title: CKmeans and FCKmeans : Two deterministic initialization procedures for Kmeans algorithm using a modified crowding distance\n",
            "LLM generated summary: content='The content discusses the CKmeans and FCKmeans deterministic initialization procedures for the K-means clustering algorithm, highlighting the popular k-means++ method for selecting centroids based on distance.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create another template"
      ],
      "metadata": {
        "id": "iHDfSLLJBQay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context. If you cannot answer the question with the context, please respond with 'I don't know':\n",
        "\n",
        "### CONTEXT\n",
        "{context}\n",
        "\n",
        "### QUESTION\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "BqLJuLwLBLda"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "primary_qa_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | base_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": prompt | primary_qa_llm, \"context\": itemgetter(\"context\")}\n",
        ")"
      ],
      "metadata": {
        "id": "H6yNs7Ydu442"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ask specific question to get response based on papers we retrieved.\n",
        "For example: What is recommended value for number of clusters in k-Means?"
      ],
      "metadata": {
        "id": "QUd4YnYNBVS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is recommended value for number of clusters in k-Means?\"\n",
        "\n",
        "result = retrieval_augmented_qa_chain.invoke({\"question\" : question})\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_gjXA2Su8Ki",
        "outputId": "44e11449-2947-41eb-8a1f-b9ffd817e133"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'response': AIMessage(content='Answer: k = 2'), 'context': [Document(page_content='cluster.\\nAccording to [13] and our experiments, k = 2 appeared to have produced the\\nbest clustering results comparing to using 3 or more clusters, when some clusters\\ncould end up containing too many extreme values and aﬀect the calculation of\\nmean and standard deviation values. According to step 2 in Algorithm 1, the\\nintra-cluster distance, from each data point to the centroid of the cluster it\\nbelongs to, is calculated. All intra-cluster distances are sorted into descending', metadata={'Published': '2019-10-15', 'Title': 'MSD-Kmeans: A Novel Algorithm for Efficient Detection of Global and Local Outliers', 'Authors': 'Yuanyuan Wei, Julian Jang-Jaccard, Fariza Sabrina, Timothy McIntosh', 'Summary': 'Outlier detection is a technique in data mining that aims to detect unusual\\nor unexpected records in the dataset. Existing outlier detection algorithms\\nhave different pros and cons and exhibit different sensitivity to noisy data\\nsuch as extreme values. In this paper, we propose a novel cluster-based outlier\\ndetection algorithm named MSD-Kmeans that combines the statistical method of\\nMean and Standard Deviation (MSD) and the machine learning clustering algorithm\\nK-means to detect outliers more accurately with the better control of extreme\\nvalues. There are two phases in this combination method of MSD-Kmeans: (1)\\napplying MSD algorithm to eliminate as many noisy data to minimize the\\ninterference on clusters, and (2) applying K-means algorithm to obtain local\\noptimal clusters. We evaluate our algorithm and demonstrate its effectiveness\\nin the context of detecting possible overcharging of taxi fares, as greedy\\ndishonest drivers may attempt to charge high fares by detouring. We compare the\\nperformance indicators of MSD-Kmeans with those of other outlier detection\\nalgorithms, such as MSD, K-means, Z-score, MIQR and LOF, and prove that the\\nproposed MSD-Kmeans algorithm achieves the highest measure of precision,\\naccuracy, and F-measure. We conclude that MSD-Kmeans can be used for effective\\nand efficient outlier detection on data of varying quality on IoT devices.'}), Document(page_content='8\\n3\\n5\\n7\\n9\\n11\\n13\\n15\\n17\\n19\\nNumber of clusters\\n0.025\\n0.050\\n0.075\\n0.100\\n0.125\\n0.150\\n0.175\\n0.200\\n0.225\\nSilhouette Score\\nInternal Validation Scores for k_means\\nBERT - CLS \\nBERT - Average \\nWord2Vec - Average \\nFigure 5: Internal validation scores for KMeans with varying number of clusters. Since the\\ninitialization centroids of the KMeans aﬀects the performance of the algorithm, we run the\\nalgorithm with starting seed values in the range of 1-50. The average results are shown here', metadata={'Published': '2023-05-04', 'Title': 'Influence of various text embeddings on clustering performance in NLP', 'Authors': 'Rohan Saha', 'Summary': 'With the advent of e-commerce platforms, reviews are crucial for customers to\\nassess the credibility of a product. The star ratings do not always match the\\nreview text written by the customer. For example, a three star rating (out of\\nfive) may be incongruous with the review text, which may be more suitable for a\\nfive star review. A clustering approach can be used to relabel the correct star\\nratings by grouping the text reviews into individual groups. In this work, we\\nexplore the task of choosing different text embeddings to represent these\\nreviews and also explore the impact the embedding choice has on the performance\\nof various classes of clustering algorithms. We use contextual (BERT) and\\nnon-contextual (Word2Vec) text embeddings to represent the text and measure\\ntheir impact of three classes on clustering algorithms - partitioning based\\n(KMeans), single linkage agglomerative hierarchical, and density based (DBSCAN\\nand HDBSCAN), each with various experimental settings. We use the silhouette\\nscore, adjusted rand index score, and cluster purity score metrics to evaluate\\nthe performance of the algorithms and discuss the impact of different\\nembeddings on the clustering performance. Our results indicate that the type of\\nembedding chosen drastically affects the performance of the algorithm, the\\nperformance varies greatly across different types of clustering algorithms, no\\nembedding type is better than the other, and DBSCAN outperforms KMeans and\\nsingle linkage agglomerative clustering but also labels more data points as\\noutliers. We provide a thorough comparison of the performances of different\\nalgorithms and provide numerous ideas to foster further research in the domain\\nof text clustering.'}), Document(page_content='gorithm. One of the crucial hyperparameters of the KMeans algorithm is the number of\\nclusters, as KMeans requires the user to specify the number of clusters in the data. The\\nreview dataset contains reviews categorized into ﬁve categories (ratings). However, this cat-\\negorization may not reﬂect itself in terms of the number of clusters in the embedding space.\\nThe question then arises, what value should we specify for the number of clusters? One of', metadata={'Published': '2023-05-04', 'Title': 'Influence of various text embeddings on clustering performance in NLP', 'Authors': 'Rohan Saha', 'Summary': 'With the advent of e-commerce platforms, reviews are crucial for customers to\\nassess the credibility of a product. The star ratings do not always match the\\nreview text written by the customer. For example, a three star rating (out of\\nfive) may be incongruous with the review text, which may be more suitable for a\\nfive star review. A clustering approach can be used to relabel the correct star\\nratings by grouping the text reviews into individual groups. In this work, we\\nexplore the task of choosing different text embeddings to represent these\\nreviews and also explore the impact the embedding choice has on the performance\\nof various classes of clustering algorithms. We use contextual (BERT) and\\nnon-contextual (Word2Vec) text embeddings to represent the text and measure\\ntheir impact of three classes on clustering algorithms - partitioning based\\n(KMeans), single linkage agglomerative hierarchical, and density based (DBSCAN\\nand HDBSCAN), each with various experimental settings. We use the silhouette\\nscore, adjusted rand index score, and cluster purity score metrics to evaluate\\nthe performance of the algorithms and discuss the impact of different\\nembeddings on the clustering performance. Our results indicate that the type of\\nembedding chosen drastically affects the performance of the algorithm, the\\nperformance varies greatly across different types of clustering algorithms, no\\nembedding type is better than the other, and DBSCAN outperforms KMeans and\\nsingle linkage agglomerative clustering but also labels more data points as\\noutliers. We provide a thorough comparison of the performances of different\\nalgorithms and provide numerous ideas to foster further research in the domain\\nof text clustering.'}), Document(page_content='be set properly for optimal clustering results. We experimented with\\nparameter settings suggested in the respective papers and reported\\nthe best results. The number of clusters in DP and KMeans was set\\nto the true number of clusters in the datasets. The parameter 𝜖 for\\nDBSCAN and DP was searched in the range of [0.01 to 0.5 with a\\nstep size of 0.01] and 𝑚𝑖𝑛𝑃𝑡𝑠 for DBSCAN in [4, 5, 6, 7, 8]. Similarly,\\ntwo parameters in ARES were searched as 𝜓 ∈ {2𝑖 |𝑖 = 0, 1, · · · , 5}', metadata={'Published': '2024-01-21', 'Title': 'Enabling clustering algorithms to detect clusters of varying densities through scale-invariant data preprocessing', 'Authors': 'Sunil Aryal, Jonathan R. Wells, Arbind Agrahari Baniya, KC Santosh', 'Summary': \"In this paper, we show that preprocessing data using a variant of rank\\ntransformation called 'Average Rank over an Ensemble of Sub-samples (ARES)'\\nmakes clustering algorithms robust to data representation and enable them to\\ndetect varying density clusters. Our empirical results, obtained using three\\nmost widely used clustering algorithms-namely KMeans, DBSCAN, and DP (Density\\nPeak)-across a wide range of real-world datasets, show that clustering after\\nARES transformation produces better and more consistent results.\"}), Document(page_content='Review on determining number of cluster in k-\\nmeans clustering. International Journal, 1(6):90–95,\\n2013.\\n[13] Andrea Lancichinetti, Santo Fortunato, and Fil-\\nippo Radicchi.\\nBenchmark graphs for testing\\ncommunity detection algorithms. Physical review\\nE, 78(4):046110, 2008.\\n[14] Robert Lund and Bo Li.\\nRevisiting climate re-\\ngion deﬁnitions via clustering. Journal of Climate,\\n22(7):1787–1800, 2009.\\n[15] Ulrike V Luxburg, Olivier Bousquet, and Mikhail', metadata={'Published': '2017-10-07', 'Title': 'A New Spectral Clustering Algorithm', 'Authors': 'W. R. Casper, Balu Nadiga', 'Summary': 'We present a new clustering algorithm that is based on searching for natural\\ngaps in the components of the lowest energy eigenvectors of the Laplacian of a\\ngraph. In comparing the performance of the proposed method with a set of other\\npopular methods (KMEANS, spectral-KMEANS, and an agglomerative method) in the\\ncontext of the Lancichinetti-Fortunato-Radicchi (LFR) Benchmark for undirected\\nweighted overlapping networks, we find that the new method outperforms the\\nother spectral methods considered in certain parameter regimes. Finally, in an\\napplication to climate data involving one of the most important modes of\\ninterannual climate variability, the El Nino Southern Oscillation phenomenon,\\nwe demonstrate the ability of the new algorithm to readily identify different\\nflavors of the phenomenon.'})]}\n"
          ]
        }
      ]
    }
  ]
}